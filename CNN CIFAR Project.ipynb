{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f6d3b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import cifar10\n",
    "import numpy as np\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dropout\n",
    "from keras.optimizers import SGD\n",
    "from keras.layers import BatchNormalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c0d6da50",
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = cifar10.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0e9eee58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[[ 59  62  63]\n",
      "   [ 43  46  45]\n",
      "   [ 50  48  43]\n",
      "   ...\n",
      "   [158 132 108]\n",
      "   [152 125 102]\n",
      "   [148 124 103]]\n",
      "\n",
      "  [[ 16  20  20]\n",
      "   [  0   0   0]\n",
      "   [ 18   8   0]\n",
      "   ...\n",
      "   [123  88  55]\n",
      "   [119  83  50]\n",
      "   [122  87  57]]\n",
      "\n",
      "  [[ 25  24  21]\n",
      "   [ 16   7   0]\n",
      "   [ 49  27   8]\n",
      "   ...\n",
      "   [118  84  50]\n",
      "   [120  84  50]\n",
      "   [109  73  42]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[208 170  96]\n",
      "   [201 153  34]\n",
      "   [198 161  26]\n",
      "   ...\n",
      "   [160 133  70]\n",
      "   [ 56  31   7]\n",
      "   [ 53  34  20]]\n",
      "\n",
      "  [[180 139  96]\n",
      "   [173 123  42]\n",
      "   [186 144  30]\n",
      "   ...\n",
      "   [184 148  94]\n",
      "   [ 97  62  34]\n",
      "   [ 83  53  34]]\n",
      "\n",
      "  [[177 144 116]\n",
      "   [168 129  94]\n",
      "   [179 142  87]\n",
      "   ...\n",
      "   [216 184 140]\n",
      "   [151 118  84]\n",
      "   [123  92  72]]]\n",
      "\n",
      "\n",
      " [[[154 177 187]\n",
      "   [126 137 136]\n",
      "   [105 104  95]\n",
      "   ...\n",
      "   [ 91  95  71]\n",
      "   [ 87  90  71]\n",
      "   [ 79  81  70]]\n",
      "\n",
      "  [[140 160 169]\n",
      "   [145 153 154]\n",
      "   [125 125 118]\n",
      "   ...\n",
      "   [ 96  99  78]\n",
      "   [ 77  80  62]\n",
      "   [ 71  73  61]]\n",
      "\n",
      "  [[140 155 164]\n",
      "   [139 146 149]\n",
      "   [115 115 112]\n",
      "   ...\n",
      "   [ 79  82  64]\n",
      "   [ 68  70  55]\n",
      "   [ 67  69  55]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[175 167 166]\n",
      "   [156 154 160]\n",
      "   [154 160 170]\n",
      "   ...\n",
      "   [ 42  34  36]\n",
      "   [ 61  53  57]\n",
      "   [ 93  83  91]]\n",
      "\n",
      "  [[165 154 128]\n",
      "   [156 152 130]\n",
      "   [159 161 142]\n",
      "   ...\n",
      "   [103  93  96]\n",
      "   [123 114 120]\n",
      "   [131 121 131]]\n",
      "\n",
      "  [[163 148 120]\n",
      "   [158 148 122]\n",
      "   [163 156 133]\n",
      "   ...\n",
      "   [143 133 139]\n",
      "   [143 134 142]\n",
      "   [143 133 144]]]\n",
      "\n",
      "\n",
      " [[[255 255 255]\n",
      "   [253 253 253]\n",
      "   [253 253 253]\n",
      "   ...\n",
      "   [253 253 253]\n",
      "   [253 253 253]\n",
      "   [253 253 253]]\n",
      "\n",
      "  [[255 255 255]\n",
      "   [255 255 255]\n",
      "   [255 255 255]\n",
      "   ...\n",
      "   [255 255 255]\n",
      "   [255 255 255]\n",
      "   [255 255 255]]\n",
      "\n",
      "  [[255 255 255]\n",
      "   [254 254 254]\n",
      "   [254 254 254]\n",
      "   ...\n",
      "   [254 254 254]\n",
      "   [254 254 254]\n",
      "   [254 254 254]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[113 120 112]\n",
      "   [111 118 111]\n",
      "   [105 112 106]\n",
      "   ...\n",
      "   [ 72  81  80]\n",
      "   [ 72  80  79]\n",
      "   [ 72  80  79]]\n",
      "\n",
      "  [[111 118 110]\n",
      "   [104 111 104]\n",
      "   [ 99 106  98]\n",
      "   ...\n",
      "   [ 68  75  73]\n",
      "   [ 70  76  75]\n",
      "   [ 78  84  82]]\n",
      "\n",
      "  [[106 113 105]\n",
      "   [ 99 106  98]\n",
      "   [ 95 102  94]\n",
      "   ...\n",
      "   [ 78  85  83]\n",
      "   [ 79  85  83]\n",
      "   [ 80  86  84]]]\n",
      "\n",
      "\n",
      " ...\n",
      "\n",
      "\n",
      " [[[ 35 178 235]\n",
      "   [ 40 176 239]\n",
      "   [ 42 176 241]\n",
      "   ...\n",
      "   [ 99 177 219]\n",
      "   [ 79 147 197]\n",
      "   [ 89 148 189]]\n",
      "\n",
      "  [[ 57 182 234]\n",
      "   [ 44 184 250]\n",
      "   [ 50 183 240]\n",
      "   ...\n",
      "   [156 182 200]\n",
      "   [141 177 206]\n",
      "   [116 149 175]]\n",
      "\n",
      "  [[ 98 197 237]\n",
      "   [ 64 189 252]\n",
      "   [ 69 192 245]\n",
      "   ...\n",
      "   [188 195 206]\n",
      "   [119 135 147]\n",
      "   [ 61  79  90]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[ 73  79  77]\n",
      "   [ 53  63  68]\n",
      "   [ 54  68  80]\n",
      "   ...\n",
      "   [ 17  40  64]\n",
      "   [ 21  36  51]\n",
      "   [ 33  48  49]]\n",
      "\n",
      "  [[ 61  68  75]\n",
      "   [ 55  70  86]\n",
      "   [ 57  79 103]\n",
      "   ...\n",
      "   [ 24  48  72]\n",
      "   [ 17  35  53]\n",
      "   [  7  23  32]]\n",
      "\n",
      "  [[ 44  56  73]\n",
      "   [ 46  66  88]\n",
      "   [ 49  77 105]\n",
      "   ...\n",
      "   [ 27  52  77]\n",
      "   [ 21  43  66]\n",
      "   [ 12  31  50]]]\n",
      "\n",
      "\n",
      " [[[189 211 240]\n",
      "   [186 208 236]\n",
      "   [185 207 235]\n",
      "   ...\n",
      "   [175 195 224]\n",
      "   [172 194 222]\n",
      "   [169 194 220]]\n",
      "\n",
      "  [[194 210 239]\n",
      "   [191 207 236]\n",
      "   [190 206 235]\n",
      "   ...\n",
      "   [173 192 220]\n",
      "   [171 191 218]\n",
      "   [167 190 216]]\n",
      "\n",
      "  [[208 219 244]\n",
      "   [205 216 240]\n",
      "   [204 215 239]\n",
      "   ...\n",
      "   [175 191 217]\n",
      "   [172 190 216]\n",
      "   [169 191 215]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[207 199 181]\n",
      "   [203 195 175]\n",
      "   [203 196 173]\n",
      "   ...\n",
      "   [135 132 127]\n",
      "   [162 158 150]\n",
      "   [168 163 151]]\n",
      "\n",
      "  [[198 190 170]\n",
      "   [189 181 159]\n",
      "   [180 172 147]\n",
      "   ...\n",
      "   [178 171 160]\n",
      "   [175 169 156]\n",
      "   [175 169 154]]\n",
      "\n",
      "  [[198 189 173]\n",
      "   [189 181 162]\n",
      "   [178 170 149]\n",
      "   ...\n",
      "   [195 184 169]\n",
      "   [196 189 171]\n",
      "   [195 190 171]]]\n",
      "\n",
      "\n",
      " [[[229 229 239]\n",
      "   [236 237 247]\n",
      "   [234 236 247]\n",
      "   ...\n",
      "   [217 219 233]\n",
      "   [221 223 234]\n",
      "   [222 223 233]]\n",
      "\n",
      "  [[222 221 229]\n",
      "   [239 239 249]\n",
      "   [233 234 246]\n",
      "   ...\n",
      "   [223 223 236]\n",
      "   [227 228 238]\n",
      "   [210 211 220]]\n",
      "\n",
      "  [[213 206 211]\n",
      "   [234 232 239]\n",
      "   [231 233 244]\n",
      "   ...\n",
      "   [220 220 232]\n",
      "   [220 219 232]\n",
      "   [202 203 215]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[150 143 135]\n",
      "   [140 135 127]\n",
      "   [132 127 120]\n",
      "   ...\n",
      "   [224 222 218]\n",
      "   [230 228 225]\n",
      "   [241 241 238]]\n",
      "\n",
      "  [[137 132 126]\n",
      "   [130 127 120]\n",
      "   [125 121 115]\n",
      "   ...\n",
      "   [181 180 178]\n",
      "   [202 201 198]\n",
      "   [212 211 207]]\n",
      "\n",
      "  [[122 119 114]\n",
      "   [118 116 110]\n",
      "   [120 116 111]\n",
      "   ...\n",
      "   [179 177 173]\n",
      "   [164 164 162]\n",
      "   [163 163 161]]]]\n"
     ]
    }
   ],
   "source": [
    "print(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "426fadaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "61bc2162",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train / 255.0\n",
    "X_test = X_test / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "52ce1f43",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\keras\\lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(32,(3,3), activation='relu', kernel_initializer='he_uniform', padding='same', input_shape=(32,32,3)))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Conv2D(32,(3,3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D((2,2)))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Conv2D(64,(3,3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Conv2D(64,(3,3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D((2,2)))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Conv2D(128,(3,3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Conv2D(128,(3,3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D((2,2)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(128,activation='relu'))\n",
    "model.add(Dense(10,activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5abca7e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "782/782 - 90s - 115ms/step - accuracy: 0.4207 - loss: 1.5974 - val_accuracy: 0.5203 - val_loss: 1.3263\n",
      "Epoch 2/50\n",
      "782/782 - 93s - 119ms/step - accuracy: 0.5648 - loss: 1.2059 - val_accuracy: 0.5893 - val_loss: 1.1490\n",
      "Epoch 3/50\n",
      "782/782 - 97s - 124ms/step - accuracy: 0.6270 - loss: 1.0374 - val_accuracy: 0.6439 - val_loss: 1.0068\n",
      "Epoch 4/50\n",
      "782/782 - 96s - 123ms/step - accuracy: 0.6688 - loss: 0.9344 - val_accuracy: 0.6987 - val_loss: 0.8617\n",
      "Epoch 5/50\n",
      "782/782 - 101s - 129ms/step - accuracy: 0.6983 - loss: 0.8514 - val_accuracy: 0.6780 - val_loss: 0.9355\n",
      "Epoch 6/50\n",
      "782/782 - 99s - 126ms/step - accuracy: 0.7205 - loss: 0.7883 - val_accuracy: 0.7127 - val_loss: 0.8315\n",
      "Epoch 7/50\n",
      "782/782 - 100s - 128ms/step - accuracy: 0.7402 - loss: 0.7360 - val_accuracy: 0.7229 - val_loss: 0.7968\n",
      "Epoch 8/50\n",
      "782/782 - 102s - 131ms/step - accuracy: 0.7557 - loss: 0.6904 - val_accuracy: 0.7309 - val_loss: 0.7730\n",
      "Epoch 9/50\n",
      "782/782 - 99s - 126ms/step - accuracy: 0.7709 - loss: 0.6502 - val_accuracy: 0.7597 - val_loss: 0.6864\n",
      "Epoch 10/50\n",
      "782/782 - 98s - 126ms/step - accuracy: 0.7821 - loss: 0.6155 - val_accuracy: 0.7454 - val_loss: 0.7399\n",
      "Epoch 11/50\n",
      "782/782 - 142s - 182ms/step - accuracy: 0.7924 - loss: 0.5844 - val_accuracy: 0.7627 - val_loss: 0.6915\n",
      "Epoch 12/50\n",
      "782/782 - 97s - 124ms/step - accuracy: 0.8043 - loss: 0.5539 - val_accuracy: 0.7793 - val_loss: 0.6466\n",
      "Epoch 13/50\n",
      "782/782 - 89s - 114ms/step - accuracy: 0.8118 - loss: 0.5307 - val_accuracy: 0.7760 - val_loss: 0.6540\n",
      "Epoch 14/50\n",
      "782/782 - 95s - 122ms/step - accuracy: 0.8199 - loss: 0.5080 - val_accuracy: 0.7886 - val_loss: 0.6146\n",
      "Epoch 15/50\n",
      "782/782 - 98s - 126ms/step - accuracy: 0.8259 - loss: 0.4900 - val_accuracy: 0.7801 - val_loss: 0.6556\n",
      "Epoch 16/50\n",
      "782/782 - 102s - 131ms/step - accuracy: 0.8359 - loss: 0.4628 - val_accuracy: 0.7992 - val_loss: 0.5971\n",
      "Epoch 17/50\n",
      "782/782 - 98s - 126ms/step - accuracy: 0.8426 - loss: 0.4440 - val_accuracy: 0.8004 - val_loss: 0.6101\n",
      "Epoch 18/50\n",
      "782/782 - 98s - 125ms/step - accuracy: 0.8477 - loss: 0.4253 - val_accuracy: 0.8027 - val_loss: 0.6008\n",
      "Epoch 19/50\n",
      "782/782 - 99s - 126ms/step - accuracy: 0.8553 - loss: 0.4066 - val_accuracy: 0.7943 - val_loss: 0.6534\n",
      "Epoch 20/50\n",
      "782/782 - 97s - 124ms/step - accuracy: 0.8592 - loss: 0.3912 - val_accuracy: 0.7869 - val_loss: 0.6660\n",
      "Epoch 21/50\n",
      "782/782 - 86s - 111ms/step - accuracy: 0.8660 - loss: 0.3728 - val_accuracy: 0.8051 - val_loss: 0.5968\n",
      "Epoch 22/50\n",
      "782/782 - 105s - 134ms/step - accuracy: 0.8725 - loss: 0.3541 - val_accuracy: 0.8111 - val_loss: 0.6106\n",
      "Epoch 23/50\n",
      "782/782 - 100s - 128ms/step - accuracy: 0.8787 - loss: 0.3404 - val_accuracy: 0.8124 - val_loss: 0.5877\n",
      "Epoch 24/50\n",
      "782/782 - 97s - 125ms/step - accuracy: 0.8846 - loss: 0.3253 - val_accuracy: 0.8112 - val_loss: 0.6086\n",
      "Epoch 25/50\n",
      "782/782 - 99s - 126ms/step - accuracy: 0.8850 - loss: 0.3181 - val_accuracy: 0.8038 - val_loss: 0.6492\n",
      "Epoch 26/50\n",
      "782/782 - 103s - 131ms/step - accuracy: 0.8931 - loss: 0.2990 - val_accuracy: 0.8124 - val_loss: 0.6126\n",
      "Epoch 27/50\n",
      "782/782 - 103s - 132ms/step - accuracy: 0.8983 - loss: 0.2834 - val_accuracy: 0.8138 - val_loss: 0.6385\n",
      "Epoch 28/50\n",
      "782/782 - 99s - 126ms/step - accuracy: 0.9037 - loss: 0.2700 - val_accuracy: 0.8186 - val_loss: 0.6117\n",
      "Epoch 29/50\n",
      "782/782 - 99s - 126ms/step - accuracy: 0.9056 - loss: 0.2641 - val_accuracy: 0.8184 - val_loss: 0.6153\n",
      "Epoch 30/50\n",
      "782/782 - 91s - 117ms/step - accuracy: 0.9117 - loss: 0.2465 - val_accuracy: 0.8198 - val_loss: 0.6125\n",
      "Epoch 31/50\n",
      "782/782 - 86s - 110ms/step - accuracy: 0.9134 - loss: 0.2381 - val_accuracy: 0.8159 - val_loss: 0.6471\n",
      "Epoch 32/50\n",
      "782/782 - 86s - 110ms/step - accuracy: 0.9177 - loss: 0.2280 - val_accuracy: 0.8231 - val_loss: 0.6102\n",
      "Epoch 33/50\n",
      "782/782 - 86s - 110ms/step - accuracy: 0.9214 - loss: 0.2175 - val_accuracy: 0.8247 - val_loss: 0.6427\n",
      "Epoch 34/50\n",
      "782/782 - 96s - 123ms/step - accuracy: 0.9227 - loss: 0.2102 - val_accuracy: 0.8270 - val_loss: 0.6345\n",
      "Epoch 35/50\n",
      "782/782 - 114s - 146ms/step - accuracy: 0.9273 - loss: 0.2027 - val_accuracy: 0.8190 - val_loss: 0.6686\n",
      "Epoch 36/50\n",
      "782/782 - 117s - 149ms/step - accuracy: 0.9288 - loss: 0.1952 - val_accuracy: 0.8237 - val_loss: 0.6690\n",
      "Epoch 37/50\n",
      "782/782 - 98s - 126ms/step - accuracy: 0.9341 - loss: 0.1837 - val_accuracy: 0.8240 - val_loss: 0.6498\n",
      "Epoch 38/50\n",
      "782/782 - 127s - 163ms/step - accuracy: 0.9357 - loss: 0.1787 - val_accuracy: 0.8104 - val_loss: 0.7106\n",
      "Epoch 39/50\n",
      "782/782 - 101s - 130ms/step - accuracy: 0.9386 - loss: 0.1675 - val_accuracy: 0.8295 - val_loss: 0.6440\n",
      "Epoch 40/50\n",
      "782/782 - 103s - 131ms/step - accuracy: 0.9401 - loss: 0.1636 - val_accuracy: 0.8268 - val_loss: 0.6588\n",
      "Epoch 41/50\n",
      "782/782 - 105s - 134ms/step - accuracy: 0.9435 - loss: 0.1555 - val_accuracy: 0.8231 - val_loss: 0.6999\n",
      "Epoch 42/50\n",
      "782/782 - 102s - 131ms/step - accuracy: 0.9445 - loss: 0.1527 - val_accuracy: 0.8225 - val_loss: 0.7020\n",
      "Epoch 43/50\n",
      "782/782 - 108s - 138ms/step - accuracy: 0.9463 - loss: 0.1491 - val_accuracy: 0.8283 - val_loss: 0.6990\n",
      "Epoch 44/50\n",
      "782/782 - 106s - 135ms/step - accuracy: 0.9492 - loss: 0.1397 - val_accuracy: 0.8228 - val_loss: 0.7050\n",
      "Epoch 45/50\n",
      "782/782 - 94s - 120ms/step - accuracy: 0.9511 - loss: 0.1342 - val_accuracy: 0.8321 - val_loss: 0.6721\n",
      "Epoch 46/50\n",
      "782/782 - 86s - 110ms/step - accuracy: 0.9546 - loss: 0.1261 - val_accuracy: 0.8292 - val_loss: 0.7100\n",
      "Epoch 47/50\n",
      "782/782 - 99s - 127ms/step - accuracy: 0.9547 - loss: 0.1244 - val_accuracy: 0.8299 - val_loss: 0.6942\n",
      "Epoch 48/50\n",
      "782/782 - 107s - 137ms/step - accuracy: 0.9567 - loss: 0.1208 - val_accuracy: 0.8232 - val_loss: 0.7530\n",
      "Epoch 49/50\n",
      "782/782 - 105s - 134ms/step - accuracy: 0.9583 - loss: 0.1124 - val_accuracy: 0.8322 - val_loss: 0.7094\n",
      "Epoch 50/50\n",
      "782/782 - 101s - 130ms/step - accuracy: 0.9581 - loss: 0.1155 - val_accuracy: 0.8324 - val_loss: 0.7160\n",
      "Accuracy of CNN model : 83.24000239372253\n"
     ]
    }
   ],
   "source": [
    "optimizer = SGD(learning_rate=0.001, momentum=0.95)\n",
    "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(X_train, y_train, epochs=50, batch_size=64, validation_data=(X_test, y_test), verbose=2)\n",
    "model.result = model.evaluate(X_test, y_test, verbose=0)\n",
    "print('Accuracy of CNN model : %s' % (model.result[1]* 100.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e9dc60c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML-KERNEL",
   "language": "python",
   "name": "ml-kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
